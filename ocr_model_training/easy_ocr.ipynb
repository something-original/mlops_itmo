{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPx9lddg6DQK"
      },
      "source": [
        "# OCR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwsVewT46Px1"
      },
      "source": [
        "### Подготовка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "colab = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-7A-d5yBxkyo",
        "outputId": "98ceb869-fde3-4401-a714-3fe70a487d0a"
      },
      "outputs": [],
      "source": [
        "if colab:\n",
        "    !pip install requests\n",
        "    !pip install random_word\n",
        "    !pip install tqdm\n",
        "    !pip install streamlit>=1.0.0\n",
        "    !pip install wandb>=0.10.31\n",
        "    !pip install matplotlib>=3.1.0\n",
        "    !pip install defusedxml\n",
        "    !pip install opencv-python-headless\n",
        "    !pip install anyascii\n",
        "    !pip install ipykernel==6.29.5\n",
        "    !pip install pandas\n",
        "    !pip install wget\n",
        "    !pip install pyyaml\n",
        "    !pip install natsort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNSRndKeyYcV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from random_word import RandomWords\n",
        "import string\n",
        "import shutil\n",
        "import wget\n",
        "import zipfile\n",
        "import yaml\n",
        "import wget\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9rdM2EAYaY0"
      },
      "source": [
        "#### Загружаем датасет с Hugging Face и распаковываем"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCH17RFdxkyq"
      },
      "source": [
        "Если нужно перегенерировать датасет, в ячейке ниже должно быть True. В таком случае загружаем кириллический датасет с HF и дополняем его своей синтетикой. Иначе загружаем готовый датасет со всеми необходимыми символами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLJrtfcYxkyq"
      },
      "outputs": [],
      "source": [
        "generate_dataset = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjiTcu13Q6_d",
        "outputId": "95b2df9e-e782-4eca-c05b-272e21911103"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting files: 100%|██████████| 500001/500001 [01:48<00:00, 4606.24it/s]\n"
          ]
        }
      ],
      "source": [
        "if generate_dataset:\n",
        "  file_name = 'data_1.zip'\n",
        "  if not os.path.exists(file_name):\n",
        "    wget.download('https://huggingface.co/datasets/DonkeySmall/OCR-Cyrillic-Printed-1/resolve/main/data_1.zip')\n",
        "  with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "    for file in tqdm(zip_ref.infolist(), desc='Extracting files'):\n",
        "        zip_ref.extract(file, 'dataset')\n",
        "else:\n",
        "   # клонируем с HF\n",
        "  wget.download('https://huggingface.co/datasets/smthrgnl/ocr_cyrillic_english/resolve/main/images.zip')\n",
        "  wget.download('https://huggingface.co/datasets/smthrgnl/ocr_cyrillic_english/resolve/main/labels.csv')\n",
        "  with zipfile.ZipFile('images.zip') as zip_ref:\n",
        "    for file in tqdm(zip_ref.infolist(), desc='Extracting files'):\n",
        "      zip_ref.extract(file, 'dataset')\n",
        "  os.remove('images.zip')\n",
        "  shutil.move('labels.csv', 'dataset/labels.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooq0GnoH5-DP"
      },
      "source": [
        "### Генерация датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya6taa8ZeWVP"
      },
      "outputs": [],
      "source": [
        "#русское слово (берем с HF)\n",
        "#русское cлово + знак препинания (берем с HF и добавляем знак) \".,?!:;/%\"\n",
        "#русское слово с дефисом внутри слова\n",
        "#число\n",
        "#число, разделенное точкой или дефисом (01.01.2024)\n",
        "#число + символ валюты или процент\n",
        "#слово или число в скобках, кавычках (с одной стороны или с 2х) \"()\"«»\"\n",
        "#имя (И.И.)\n",
        "#№ + числа\n",
        "#английское слово\n",
        "#электронная почта"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5esvww02v9dZ"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('arial-cyr.ttf') and generate_dataset:\n",
        "  wget.download('https://huggingface.co/datasets/smthrgnl/arial_cyrillic_font/blob/main/arial-cyr.ttf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrAgeS5o2HdA"
      },
      "source": [
        "Добавляем синтетический датасет"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_bzu6VBiTEw"
      },
      "outputs": [],
      "source": [
        "def generate_image_for_word(word, output_directory, image_name):\n",
        "\n",
        "    font_size = random.randint(50, 100)\n",
        "    padding = 20\n",
        "    vertical_padding = 30\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
        "    except IOError:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"arialbd.ttf\", font_size)\n",
        "        except IOError:\n",
        "            font = ImageFont.load_default()\n",
        "            font.size = font_size\n",
        "\n",
        "    temp_img = Image.new('RGB', (1, 1))\n",
        "    temp_draw = ImageDraw.Draw(temp_img)\n",
        "\n",
        "    bbox = temp_draw.textbbox((0, 0), word, font=font)\n",
        "    text_width = bbox[2] - bbox[0]\n",
        "    text_height = bbox[3] - bbox[1]\n",
        "\n",
        "    ascent, descent = font.getmetrics()\n",
        "\n",
        "    img_width = text_width + 2 * padding\n",
        "    img_height = ascent + descent + vertical_padding\n",
        "\n",
        "    bg_color = (random.randint(110, 255), random.randint(110, 255), random.randint(110, 255))\n",
        "    text_color = (random.randint(0, 100), random.randint(0, 100), random.randint(0, 100))\n",
        "\n",
        "    img = Image.new('RGB', (img_width, img_height), color=bg_color)\n",
        "    d = ImageDraw.Draw(img)\n",
        "\n",
        "    text_position = (padding, (img_height - text_height) // 2 - descent // 2)\n",
        "\n",
        "    d.text(text_position, word, fill=text_color, font=font)\n",
        "\n",
        "    image_path = os.path.join(output_directory, image_name)\n",
        "    img.save(image_path, 'JPEG')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfCdZqsafYNY"
      },
      "outputs": [],
      "source": [
        "r = RandomWords()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vgkH7pD35Cp"
      },
      "outputs": [],
      "source": [
        "punctuation = \".,?!:;/\"\n",
        "currency = \"$\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqp6L1Q8YDun"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "\n",
        "if generate_dataset:\n",
        "\n",
        "  words_df = pd.DataFrame(columns=['filename', 'words'])\n",
        "  filenames, words = [], []\n",
        "\n",
        "  if 'images' in os.listdir('dataset'):\n",
        "    shutil.rmtree('dataset/images')\n",
        "  os.makedirs('dataset/images')\n",
        "\n",
        "  with open('./dataset/1.txt', 'r', encoding='utf-8') as labels_txt:\n",
        "\n",
        "      counter = 0\n",
        "\n",
        "      for line in tqdm(labels_txt.readlines()):\n",
        "\n",
        "        #if counter == 10:\n",
        "        #  break\n",
        "\n",
        "        rand = random.randint(1, 36)\n",
        "        splitted_line = line.split('`')\n",
        "        img_name = splitted_line[0][2:]\n",
        "        img_value = splitted_line[1][:-1] if splitted_line[1][-1] == '\\n' else splitted_line[1]\n",
        "\n",
        "        if rand in range(1, 8): #обычное слово на русском языке\n",
        "            shutil.copy(f'dataset/1/{img_name}', f'dataset/images/{img_name}')\n",
        "        else:\n",
        "          if rand in range(9, 12): #слово со знаком препинания\n",
        "            img_value = img_value + random.choice(punctuation)\n",
        "\n",
        "          elif rand in range(13, 15): #слово с дефисом\n",
        "            index = random.randint(1, len(img_value) - 2)\n",
        "            img_value = img_value[:index] + '-' + img_value[index:]\n",
        "\n",
        "          elif rand in range(16, 18): #число\n",
        "            img_value = ''.join(random.choices(string.digits, k=random.randint(1, 12)))\n",
        "\n",
        "          elif rand in range(19, 21): #число с разделителями\n",
        "            num = ''.join(random.choices(string.digits, k=random.randint(5, 12)))\n",
        "            n = random.randint(0, 1)\n",
        "            sep = '.' if n == 0 else '-'\n",
        "            index1 = random.randint(1, len(num) // 2)\n",
        "            index2 = random.randint(len(num) // 2 + 1, len(num) - 2)\n",
        "            img_value = ''.join(num[:index1]) + sep + ''.join(num[index1:index2]) + sep + ''.join(num[index2:])\n",
        "\n",
        "          elif rand in range(22, 24): #число + символ валюты, номер или процент\n",
        "            num = ''.join(random.choices(string.digits, k=random.randint(1, 6)))\n",
        "            n = random.randint(0, 2)\n",
        "            if n == 0:\n",
        "              suffix = '%'\n",
        "              img_value = num + suffix\n",
        "            elif n == 1:\n",
        "              suffix = currency\n",
        "              img_value = num + suffix\n",
        "            else:\n",
        "              img_value = '№' + num\n",
        "\n",
        "          elif rand in range(25, 27): #объект в скобках\n",
        "            n = random.randint(0, 8)\n",
        "            if n == 0:\n",
        "              img_value = '(' + img_value\n",
        "            elif n == 1:\n",
        "              img_value = img_value + ')'\n",
        "            elif n == 2:\n",
        "              img_value = '(' + img_value + ')'\n",
        "            elif n == 3:\n",
        "              img_value = '«' + img_value\n",
        "            elif n == 4:\n",
        "              img_value = img_value + '»'\n",
        "            elif n == 5:\n",
        "              img_value = '«' + img_value + '»'\n",
        "            elif n == 6:\n",
        "              img_value = '\\\"' + img_value\n",
        "            elif n == 7:\n",
        "              img_value = img_value + '\\\"'\n",
        "            elif n == 8:\n",
        "              img_value = '\\\"' + img_value + '\\\"'\n",
        "\n",
        "\n",
        "          elif rand in range(28, 29): # инициалы (2 заглавных буквы с точками)\n",
        "            img_value = '.'.join([random.choice('АБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЭЮЯ') for _ in range(2)]) + '.'\n",
        "\n",
        "          elif rand in range(30, 34): # английское слово\n",
        "            eng_word = r.get_random_word()\n",
        "            seed = random.randint(1, 3)\n",
        "            if seed == 2:\n",
        "              eng_word = eng_word[0].upper() + eng_word[1:]\n",
        "            if seed == 3:\n",
        "              eng_word = eng_word.upper()\n",
        "            if len(eng_word) > 31:\n",
        "              eng_word = eng_word[0:31]\n",
        "            img_value = eng_word\n",
        "\n",
        "          elif rand in range(35, 36): # почтовый адрес (в рандомное место вставляем _)\n",
        "            mail = r.get_random_word()\n",
        "            if len(mail) > 13:\n",
        "              mail = mail[0:12]\n",
        "            index = random.randint(1, len(mail))\n",
        "            server = r.get_random_word()\n",
        "            if len(server) > 13:\n",
        "              server = server[0:12]\n",
        "            k = random.randint(1, 3)\n",
        "            if k == 3:\n",
        "              mail = mail[:index] + '_' + mail[index:]\n",
        "            domain = ''.join(random.choices(string.ascii_lowercase, k=random.randint(2, 3)))\n",
        "            img_value = mail + '@' + server + '.' + domain\n",
        "\n",
        "          img_name = img_name.replace('_1_', '_2_')\n",
        "          fs = random.randint(22, 45)\n",
        "          generate_image_for_word(word=img_value, image_name=img_name, output_directory='./dataset/images')\n",
        "\n",
        "          if img_value[0] == '\\\"':\n",
        "            img_value = '\\\\' + img_value\n",
        "          if img_value[-1] == '\\\"':\n",
        "            img_value = img_value[:-1] + '\\\\' + '\\\"'\n",
        "\n",
        "        filenames.append(img_name)\n",
        "        words.append(img_value)\n",
        "        counter += 1\n",
        "\n",
        "  words_df['filename'] = filenames\n",
        "  words_df['words'] = words\n",
        "  words_df.to_csv('dataset/labels.csv', sep=';', index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hZJkLAxY3ai"
      },
      "source": [
        "Делим на тренировочную, тестовую, валидационную выборки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eqsUzIwYwUy"
      },
      "outputs": [],
      "source": [
        "def split_images(labels_file_path, image_folder_path, ranges):\n",
        "\n",
        "    labels_df = pd.read_csv(labels_file_path, sep=';')\n",
        "\n",
        "    for dir in ['train', 'val', 'test_']:\n",
        "        images_dir = f'./{dir}/images'\n",
        "        if os.path.exists(images_dir):\n",
        "            shutil.rmtree(images_dir)\n",
        "        os.makedirs(images_dir)\n",
        "\n",
        "    for i, (start, end) in enumerate(ranges):\n",
        "\n",
        "        start_index = start\n",
        "        end_index = end\n",
        "        if i == 0:\n",
        "            path = './train'\n",
        "        if i == 1:\n",
        "            path = './val'\n",
        "        if i == 2:\n",
        "            path = './test_'\n",
        "\n",
        "        output_file = f'{path}/labels.csv'\n",
        "        labels_part = labels_df.iloc[start:end]\n",
        "        labels_part.to_csv(output_file, sep=' ', index=False)\n",
        "        print(f'Written lines {start} to {end} to {output_file}')\n",
        "\n",
        "        # Copy the specified range of files\n",
        "        for i in tqdm(range(start_index, end_index)):\n",
        "\n",
        "            img_filename = labels_df['filename'].iloc[i]\n",
        "\n",
        "            src_file = os.path.join(image_folder_path, img_filename)\n",
        "            dest_file = os.path.join(f'{path}/images', img_filename)\n",
        "\n",
        "            # Copy the file\n",
        "            shutil.copy2(src_file, dest_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH47RSwxcQ21"
      },
      "source": [
        "Задаем размеры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2eoUzg3b7RU"
      },
      "outputs": [],
      "source": [
        "labels_file = './dataset/labels.csv'\n",
        "images_path = './dataset/images'\n",
        "line_ranges = [(0, 30000), (30000, 40000), (40000, 50000)] #train range, val range, test range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dC3OHHfcO_7",
        "outputId": "b3a3945b-def8-4ce0-c571-90c725dfc8cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Written lines 0 to 30000 to ./train/labels.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [00:25<00:00, 1165.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Written lines 30000 to 40000 to ./val/labels.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:02<00:00, 3661.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Written lines 40000 to 50000 to ./test_/labels.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:02<00:00, 3501.33it/s]\n"
          ]
        }
      ],
      "source": [
        "split_dataset = True\n",
        "if split_dataset:\n",
        "  split_images(labels_file, images_path, line_ranges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4pK6jS_lK26"
      },
      "source": [
        "### Обучение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkjd1pXGxkyu"
      },
      "source": [
        "Клонируем библиотеку и копируем датасет в нужные директории"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsVpBZBgengy"
      },
      "outputs": [],
      "source": [
        "if 'EasyOCR' not in os.listdir():\n",
        "  subprocess.run(['git', 'clone', 'https://github.com/something-original/EasyOCR'])\n",
        "  shutil.move('./train', './EasyOCR/trainer/all_data', copy_function=shutil.copy2)\n",
        "  shutil.move('./val', './EasyOCR/trainer/all_data', copy_function=shutil.copy2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvlOcSWnxkyv"
      },
      "source": [
        "Загружаем предобученный чекпоинт"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DHEX_V5xkyv"
      },
      "outputs": [],
      "source": [
        "if 'cyrillic_g2.pth' not in os.listdir():\n",
        "  wget.download('https://huggingface.co/smthrgnl/easy_ocr/blob/main/cyrillic_g2.pth')\n",
        "if 'model' not in os.listdir('EasyOCR'):\n",
        "  os.makedirs('EasyOCR/model')\n",
        "  shutil.move('./cyrillic_g2.pth', 'EasyOCR/model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8p57S2txkyv"
      },
      "source": [
        "Настраиваем параметры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7YXW1wDxkyv"
      },
      "outputs": [],
      "source": [
        "symbols = '.,?!:;/%-№()«»\\$_@' + '\\'' + '\\\"'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob9j6Awrxkyv"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "  'number': '0123456789',\n",
        "  'symbol': symbols,\n",
        "  'lang_char': 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюя',\n",
        "  'experiment_name': 'fmd',\n",
        "  'train_data': 'all_data/train',\n",
        "  'valid_data': 'all_data/val',\n",
        "  'manualSeed': 1111,\n",
        "  'workers': 6,\n",
        "  'batch_size': 64, #32\n",
        "  'num_iter': 30000,\n",
        "  'valInterval': 200,\n",
        "  'saved_model': 'cyrillic_g2',\n",
        "  'FT': True,\n",
        "  'optim': False, # значение по умолчанию - Adadelta\n",
        "  'lr': 1.,\n",
        "  'beta1': 0.9,\n",
        "  'rho': 0.95,\n",
        "  'eps': 0.00000001,\n",
        "  'grad_clip': 5,\n",
        "\n",
        "  'select_data': 'train', # это папка dataset в train_data\n",
        "  'batch_ratio': '1',\n",
        "  'total_data_usage_ratio': 1.0,\n",
        "  'batch_max_length': 68,\n",
        "  'imgH': 64,\n",
        "  'imgW': 600,\n",
        "  'rgb': False,\n",
        "  'contrast_adjust': False,\n",
        "  'sensitive': True,\n",
        "  'PAD': True,\n",
        "  'contrast_adjust': 0.0,\n",
        "  'data_filtering_off': False,\n",
        "\n",
        "  'Transformation': 'None',\n",
        "  'FeatureExtraction': 'VGG',\n",
        "  'SequenceModeling': 'BiLSTM',\n",
        "  'Prediction': 'CTC',\n",
        "  'num_fiducial': 20,\n",
        "  'input_channel': 1,\n",
        "  'output_channel': 256,\n",
        "  'hidden_size': 256,\n",
        "  'decode': 'greedy',\n",
        "  'new_prediction': False,\n",
        "  'freeze_FeatureFxtraction': False,\n",
        "  'freeze_SequenceModeling': False\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWI9agYmxkyv"
      },
      "outputs": [],
      "source": [
        "with open('EasyOCR/trainer/config_files/custom_data_train.yaml', 'w') as file:\n",
        "    yaml.dump(params, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRKJC1zUzhkk"
      },
      "source": [
        "#### Запуск обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cJ4SkJmzxkyw",
        "outputId": "de33f4e3-46f8-4f5f-a12e-90b7b66f6f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r EasyOCR/requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from -r EasyOCR/requirements.txt (line 2)) (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from -r EasyOCR/requirements.txt (line 3)) (4.11.0.86)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r EasyOCR/requirements.txt (line 4)) (1.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r EasyOCR/requirements.txt (line 5)) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from -r EasyOCR/requirements.txt (line 6)) (11.1.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from -r EasyOCR/requirements.txt (line 7)) (0.25.2)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.11/dist-packages (from -r EasyOCR/requirements.txt (line 8)) (0.6.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from -r EasyOCR/requirements.txt (line 9)) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from -r EasyOCR/requirements.txt (line 10)) (2.1.0)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.11/dist-packages (from -r EasyOCR/requirements.txt (line 11)) (1.3.0.post6)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from -r EasyOCR/requirements.txt (line 12)) (1.11.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r EasyOCR/requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r EasyOCR/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r EasyOCR/requirements.txt (line 7)) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r EasyOCR/requirements.txt (line 7)) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r EasyOCR/requirements.txt (line 7)) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r EasyOCR/requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r EasyOCR/requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (8.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r EasyOCR/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7SOasnaxkyw"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.insert(0, './EasyOCR/trainer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFHA72q5egKS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch.backends.cudnn as cudnn\n",
        "import yaml\n",
        "from train import train\n",
        "from utils import AttrDict\n",
        "import pandas as pd\n",
        "\n",
        "cudnn.benchmark = True\n",
        "cudnn.deterministic = False\n",
        "\n",
        "def get_config(file_path):\n",
        "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
        "        opt = yaml.safe_load(stream)\n",
        "    opt = AttrDict(opt)\n",
        "    if opt.lang_char == 'None':\n",
        "        characters = ''\n",
        "        for data in opt['select_data'].split('-'):\n",
        "            csv_path = os.path.join(opt['train_data'], data, 'labels.csv')\n",
        "            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
        "            all_char = ''.join(df['words'])\n",
        "            characters += ''.join(set(all_char))\n",
        "        characters = sorted(set(characters))\n",
        "        opt.character= ''.join(characters)\n",
        "    else:\n",
        "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
        "    os.makedirs(f'./saved_models/{opt.experiment_name}', exist_ok=True)\n",
        "    return opt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6F9oZGDUPbo",
        "outputId": "60e0b665-fa3f-4668-c3c9-3cfae4aed7d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'FT': True,\n",
              " 'FeatureExtraction': 'VGG',\n",
              " 'PAD': True,\n",
              " 'Prediction': 'CTC',\n",
              " 'SequenceModeling': 'BiLSTM',\n",
              " 'Transformation': 'None',\n",
              " 'batch_max_length': 68,\n",
              " 'batch_ratio': ['1'],\n",
              " 'batch_size': 64,\n",
              " 'beta1': 0.9,\n",
              " 'contrast_adjust': 0.0,\n",
              " 'data_filtering_off': False,\n",
              " 'decode': 'greedy',\n",
              " 'eps': 1e-08,\n",
              " 'experiment_name': 'fmd',\n",
              " 'freeze_FeatureFxtraction': False,\n",
              " 'freeze_SequenceModeling': False,\n",
              " 'grad_clip': 5,\n",
              " 'hidden_size': 256,\n",
              " 'imgH': 64,\n",
              " 'imgW': 600,\n",
              " 'input_channel': 1,\n",
              " 'lang_char': 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюя',\n",
              " 'lr': 1.0,\n",
              " 'manualSeed': 1111,\n",
              " 'new_prediction': False,\n",
              " 'num_fiducial': 20,\n",
              " 'num_iter': 30000,\n",
              " 'number': '0123456789',\n",
              " 'optim': False,\n",
              " 'output_channel': 256,\n",
              " 'rgb': False,\n",
              " 'rho': 0.95,\n",
              " 'saved_model': 'cyrillic_g2',\n",
              " 'select_data': ['train'],\n",
              " 'sensitive': True,\n",
              " 'symbol': '.,?!:;/%-№()«»\\\\$_@\\'\"',\n",
              " 'total_data_usage_ratio': 1.0,\n",
              " 'train_data': 'all_data/train',\n",
              " 'valInterval': 200,\n",
              " 'valid_data': 'all_data/val',\n",
              " 'workers': 6,\n",
              " 'character': '0123456789.,?!:;/%-№()«»\\\\$_@\\'\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюя'}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "Lkgozowuxkyw",
        "outputId": "b232af1c-c350-466a-d1f9-073fa8ef5e61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering the images containing characters which are not in opt.character\n",
            "Filtering the images whose label is longer than opt.batch_max_length\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root: all_data/train\n",
            "opt.select_data: ['train']\n",
            "opt.batch_ratio: ['1']\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root:    all_data/train\t dataset: train\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "datasets should not be an empty iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-ee34f644156a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Запускаем обучение\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EasyOCR/trainer/config_files/custom_data_train.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/./EasyOCR/trainer/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opt, show_number, amp)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_ratio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatch_Balanced_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./saved_models/{opt.experiment_name}/log_dataset.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/./EasyOCR/trainer/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, opt)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdashed_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdashed_line\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0m_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dataset_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhierarchical_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_d\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mtotal_number_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dataset_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/./EasyOCR/trainer/dataset.py\u001b[0m in \u001b[0;36mhierarchical_dataset\u001b[0;34m(root, opt, select_data)\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mdataset_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mconcatenated_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenated_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, datasets)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"datasets should not be an empty iterable\"\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             assert not isinstance(\n",
            "\u001b[0;31mAssertionError\u001b[0m: datasets should not be an empty iterable"
          ]
        }
      ],
      "source": [
        "#Запускаем обучение\n",
        "opt = get_config(\"EasyOCR/trainer/config_files/custom_data_train.yaml\")\n",
        "train(opt, amp=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKJk4m1dROG4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
